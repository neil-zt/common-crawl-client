{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and Decode Common Crawl Data\n",
    "\n",
    "> Please follow the instructions below to set up the environment before using the code.\n",
    "\n",
    "## Setup\n",
    "### Install Dependencies \n",
    "```zsh\n",
    "pip install pandas\n",
    "pip install comcrawl \n",
    "pip install beautifulsoup4\n",
    "```\n",
    "### Update the `comcrawl` package \n",
    "Common Crawl has updated their file download path. Please locate the downloaded `comcrawl` library directory. This can be done by holding `ctrl` and clicking the `comcrawl` keyword. Open `utils/download.py`, and replace the following identifiers with the provided code:\n",
    "- `download_multiple_results`\n",
    "  ```python\n",
    "  def download_multiple_results(results: ResultList, threads: int = None) -> ResultList:\n",
    "    # multi-threaded download\n",
    "    if threads:\n",
    "        multithreaded_download = make_multithreaded(download_single_result, threads)\n",
    "        results_with_html = multithreaded_download(results)\n",
    "    # single-threaded download\n",
    "    else:\n",
    "        for result in results:\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    result_with_html = download_single_result(result)\n",
    "                    results_with_html.append(result_with_html)\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    print(\"Library Error: download_single_result failed, retrying...\")\n",
    "    return results_with_html\n",
    "  ```\n",
    "- `URL_TEMPLATE`\n",
    "  ```python\n",
    "  https://data.commoncrawl.org/{filename}\n",
    "  ``` \n",
    "\n",
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify crawl data range and source\n",
    "Here you specify the `time_code` and the `searching_uri`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling time codes can be accessed at:\n",
    "# https://commoncrawl.org/the-data/get-started/\n",
    "time_code = \"2022-05\"\n",
    "searching_uri = \"www.cna.com.tw/news/afe/*\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comcrawl import IndexClient\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searching_uri_dir = searching_uri.replace(\"/\", \"-\").replace(\"*\", \"-all\")\n",
    "if not os.path.exists(\"output\"):\n",
    "    os.makedirs(\"output\")\n",
    "if not os.path.exists(f\"output/{time_code}\"):\n",
    "    os.makedirs(f\"output/{time_code}\")\n",
    "if not os.path.exists(f\"output/{time_code}/{searching_uri_dir}\"):\n",
    "    os.makedirs(f\"output/{time_code}/{searching_uri_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Pages from Index\n",
    "Note that the Common Crawl Index Server is constantly under heavy load and ofter responds with `504` timeout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = IndexClient([time_code], verbose=True)\n",
    "success = False\n",
    "while not success:\n",
    "    try:\n",
    "        client.search(searching_uri)\n",
    "        client.results = (pd.DataFrame(client.results)\n",
    "                        .sort_values(by=\"timestamp\")\n",
    "                        .drop_duplicates(\"urlkey\", keep=\"last\")\n",
    "                        .to_dict(\"records\"))\n",
    "        pd.DataFrame(client.results).to_csv(f\"output/{time_code}/{searching_uri_dir}/index.csv\", index=False)\n",
    "        success = True\n",
    "    except:\n",
    "        print(\"Index Server Response Timeout. Retrying...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the files \n",
    "Depending on the number of webpages and the pages' sizes, this can take up to a few hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = False\n",
    "while not success:\n",
    "    try:\n",
    "        client.download()\n",
    "        success = True\n",
    "    except:\n",
    "        print(\"Download Server Error. Retrying...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Write Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_parse import extract_main_paragraphs\n",
    "for result in client.results:\n",
    "    extracted_text = extract_main_paragraphs(result[\"html\"])\n",
    "    if len(extracted_text) < 100:\n",
    "        continue\n",
    "    with open(f\"output/{time_code}/{searching_uri_dir}/{result['urlkey'].replace('/', '-')}.txt\", \"w\") as f:\n",
    "        f.write(extracted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
